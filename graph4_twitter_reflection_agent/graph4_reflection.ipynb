{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflection Technique with LangGraph\n",
    "\n",
    "In this notebook, we'll explore the reflection technique using LangGraph. We'll create a system where one agent reflects on a tweet and provides feedback, while another agent generates an improved tweet based on that reflection. We'll use the Gemma 2 open weights model for our language tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "First, let's install the necessary packages: langchain, langchain-ollama, and langgraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain langchain-ollama langgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "Now, let's import the necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "import operator\n",
    "from typing import TypedDict, Annotated, Dict, Any\n",
    "from dotenv import load_dotenv\n",
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Prompts and Chains\n",
    "\n",
    "Now, let's define our prompts for reflection and generation, and create our language model chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reflection_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "    You are a viral twitter influencer grading a tweet. Generate critique and recommendations for the user's tweet.\n",
    "    Always provide detailed recommendations, including requests for length, virality, style, etc.,\n",
    "    \"Always return your answer in markdown format\n",
    "\n",
    "    ```markdown\n",
    "    **Original Tweet:** Just finished my first day back at work after vacation. Feeling kinda blah\n",
    "    \n",
    "    Answer:\n",
    "    **Grade:** 5/10\n",
    "\n",
    "    **Critique:** This tweet is a bit too generic. It lacks a hook to grab attention and doesn't really give us any insight\n",
    "    into your vacation or how you're feeling.\n",
    "\n",
    "    **Recommendations:**\n",
    "    - **Length:** Keep it under 280 characters for maximum impact.\n",
    "    - **Virality:** Try adding a question or call to action to encourage engagement. For example, \"What's the best thing\n",
    "    you did on your last vacation?\"\n",
    "    - **Style:** Use more descriptive language to paint a picture for your followers. Instead of 'kinda blah,' try\n",
    "    something like 'overwhelmed by the return to reality' or 'missing the sunshine and cocktails.'\n",
    "\n",
    "    **Original Tweet:**: {tweet}\n",
    "    \"\"\")\n",
    "])\n",
    "\n",
    "generation_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "    You are a twitter techie influencer assistant tasked with revising twitter posts according\n",
    "    to feedback from the previous message.\n",
    "    Revise the current tweet according to the critique to the best twitter post possible.\n",
    "    {critique}\n",
    "    You ONLY should respond with a revised version of the tweet.\n",
    "    **current tweet:** {tweet}\n",
    "    There is no need for markdown format, only write the tweet and do not write the critique\n",
    "    **New Tweet:** \n",
    "    \"\"\")\n",
    "])\n",
    "\n",
    "llm = ChatOllama(model=\"gemma2\")\n",
    "generate_chain = generation_prompt | llm\n",
    "reflect_chain = reflection_prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the State and Node Functions\n",
    "\n",
    "Let's define our State type and the functions for our reflection and generation nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    tweet: str\n",
    "    original_tweet: str\n",
    "    critique: str\n",
    "    revised_tweet: str\n",
    "    iterations: int\n",
    "\n",
    "REFLECT = \"reflect\"\n",
    "GENERATE = \"generate\"\n",
    "\n",
    "def generation_node(state: State) -> Dict[str, Any]:\n",
    "    res = generate_chain.invoke({\"tweet\": state[\"tweet\"], \"critique\": state[\"critique\"]})\n",
    "    return {\"revised_tweet\": res.content, \"iterations\": int(state[\"iterations\"]) + 1}\n",
    "\n",
    "def reflection_node(state: State) -> Dict[str, Any]:\n",
    "    res = reflect_chain.invoke({\"tweet\": state[\"tweet\"]})\n",
    "    return {\"critique\": res.content}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Graph\n",
    "\n",
    "Now, let's build our LangGraph using the StateGraph builder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(State)\n",
    "builder.add_node(REFLECT, reflection_node)\n",
    "builder.set_entry_point(REFLECT)\n",
    "\n",
    "builder.add_node(GENERATE, generation_node)\n",
    "\n",
    "def should_continue(state: Dict[str, Any]):\n",
    "    if int(state[\"iterations\"]) >= 2:\n",
    "        return END\n",
    "    return GENERATE\n",
    "\n",
    "builder.add_conditional_edges(REFLECT, should_continue)\n",
    "builder.add_edge(GENERATE, REFLECT)\n",
    "\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Graph\n",
    "\n",
    "Let's visualize our graph structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.get_graph().draw_mermaid_png(output_file_path=\"graph.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Graph\n",
    "\n",
    "Finally, let's run our graph with an initial tweet and observe the reflection and generation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"Hello LangGraph\")\n",
    "    tweet = \"\"\"\n",
    "    @LangChainAI\n",
    "    â€” newly Tool Calling feature is underrated.\n",
    "    After a long wait, it's  here- making the implementation of agents across different models\n",
    "    with function calling - super easy.\n",
    "    \"\"\"\n",
    "\n",
    "    for state in graph.stream(\n",
    "        {\"tweet\": tweet, \"revised_tweet\": tweet, \"iterations\": 0},\n",
    "        {\"configurable\": {\"thread_id\": \"odsc_userid_12844\"}},\n",
    "        stream_mode=\"values\",\n",
    "    ):\n",
    "        print(f\"{state=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've implemented a reflection technique using LangGraph. We created a loop where one agent reflects on a tweet and provides feedback, while another agent generates an improved tweet based on that reflection. The process continues for a set number of iterations, demonstrating how we can manage state and create complex workflows with LangGraph.\n",
    "\n",
    "Key points to note:\n",
    "1. We use two different prompts: one for reflection and one for generation.\n",
    "2. The state management allows us to keep track of the original tweet, critiques, revised tweets, and the number of iterations.\n",
    "3. The conditional edge allows us to control the flow of the graph based on the number of iterations.\n",
    "4. We use the Gemma 2 open weights model, showcasing the flexibility of LangGraph in working with different language models.\n",
    "\n",
    "This technique can be extended to various applications where iterative improvement and reflection are valuable, such as content creation, code review, or any task that benefits from multiple rounds of feedback and refinement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
